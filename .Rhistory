#trn_idx = createDataPartition(salaries$level, p = 0.5, list = F)
trn = salaries[trn_idx, ]
tst = salaries[-trn_idx, ]
#Start with additive model with all predictors. Log response to improve normality
salaries_model = lm(log(totalyearlycompensation) ~ ., data = trn)
#Perform backwards AIC to select significant predictors
salaries_model_back_aic = step(salaries_model, direction = "backward", trace = 0)
#Check for collinearity
car::vif(salaries_model_back_aic) #Nothing too bad
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Run diagnostics
summary(aic_model_fix)$adj
#get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
#Plot diagnostics
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
#Plotting percent error
actual_salary = tst$totalyearlycompensation
pred_salary = exp(predict(aic_model_fix, tst))
max_salary = ifelse(max(actual_salary) > max(pred_salary), max(actual_salary), max(pred_salary))
percent_error = mean((abs(pred_salary - actual_salary) / pred_salary)) * 100
plot(pred_salary, actual_salary,
col = "darkgrey",
xlab = "Predicted Salaries Prices",
ylab = "Actual Salaries",
main = "Predicted vs. Actual Salaries",
xlim = c(0, max_salary),
ylim = c(0, max_salary)
)
abline(0, 1, col = "dodgerblue")
#Simulation
num_sims = 1000
rmse_trn = rep(0, num_sims)
rmse_tst = rep(0, num_sims)
rmse = function(y, y_hat){
sqrt(sum((y - y_hat) ^ 2) / length(y))
}
for(i in 1:num_sims){
sim_fit = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
rmse_trn[i] = rmse(trn$totalyearlycompensation, exp(predict(sim_fit, trn)))
rmse_tst[i] = rmse(tst$totalyearlycompensation, exp(predict(sim_fit, tst)))
}
mean(rmse_trn)
mean(rmse_tst)
#Predict salaries
#salary_1 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 0, gender = "Female", Race = "Asian", Education = "Master's Degree") #150000
#salary_2 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 2, gender = "Male", Race = "Hispanic", Education = "Bachelor's Degree") #152000
#salary_3 = data.frame(company = "Google", level = "L5", title = "Software Engineer", #yearsofexperience = 9, gender = "Male", Race = "Asian", Education = "PhD") #396000
#exp(predict(aic_model_fix, newdata = salary_1))
#exp(predict(aic_model_fix, newdata = salary_2))
#exp(predict(aic_model_fix, newdata = salary_3))
library(car)
library(lmtest)
library(boot)
library(caret)
set.seed(22)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#trn_idx = sample(1:nrow(salaries), 1608)
trn_idx = createDataPartition(salaries$level, p = 0.7, list = F)
trn = salaries[trn_idx, ]
tst = salaries[-trn_idx, ]
#Start with additive model with all predictors. Log response to improve normality
salaries_model = lm(log(totalyearlycompensation) ~ ., data = trn)
#Perform backwards AIC to select significant predictors
salaries_model_back_aic = step(salaries_model, direction = "backward", trace = 0)
#Check for collinearity
car::vif(salaries_model_back_aic) #Nothing too bad
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Run diagnostics
summary(aic_model_fix)$adj
#get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
#Plot diagnostics
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
#Plotting percent error
actual_salary = tst$totalyearlycompensation
pred_salary = exp(predict(aic_model_fix, tst))
max_salary = ifelse(max(actual_salary) > max(pred_salary), max(actual_salary), max(pred_salary))
percent_error = mean((abs(pred_salary - actual_salary) / pred_salary)) * 100
plot(pred_salary, actual_salary,
col = "darkgrey",
xlab = "Predicted Salaries Prices",
ylab = "Actual Salaries",
main = "Predicted vs. Actual Salaries",
xlim = c(0, max_salary),
ylim = c(0, max_salary)
)
abline(0, 1, col = "dodgerblue")
#Simulation
num_sims = 1000
rmse_trn = rep(0, num_sims)
rmse_tst = rep(0, num_sims)
rmse = function(y, y_hat){
sqrt(sum((y - y_hat) ^ 2) / length(y))
}
for(i in 1:num_sims){
sim_fit = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
rmse_trn[i] = rmse(trn$totalyearlycompensation, exp(predict(sim_fit, trn)))
rmse_tst[i] = rmse(tst$totalyearlycompensation, exp(predict(sim_fit, tst)))
}
mean(rmse_trn)
mean(rmse_tst)
#Predict salaries
#salary_1 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 0, gender = "Female", Race = "Asian", Education = "Master's Degree") #150000
#salary_2 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 2, gender = "Male", Race = "Hispanic", Education = "Bachelor's Degree") #152000
#salary_3 = data.frame(company = "Google", level = "L5", title = "Software Engineer", #yearsofexperience = 9, gender = "Male", Race = "Asian", Education = "PhD") #396000
#exp(predict(aic_model_fix, newdata = salary_1))
#exp(predict(aic_model_fix, newdata = salary_2))
#exp(predict(aic_model_fix, newdata = salary_3))
coef(salaries_model_fix)
coef(aic_model_fix)
# Create a model with transformed predictors
salaries_model_res_trans= lm(log(totalyearlycompensation) ~ yearsofexperience, data = salaries)
summary(salaries_model_res_trans)
# Plot the data
plot(log(totalyearlycompensation) ~ yearsofexperience, data = salaries,
xlab = "Years of Experience",
ylab = "Log of Total Yearly Compensation",
main = "Scatter plot with fitted line for log-transformed response model")
# Add the fitted line
abline(salaries_model_res_trans, col = "red")
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(salaries_model_res_trans), resid(salaries_model_res_trans), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(salaries_model_res_trans), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(salaries_model_res_trans), col = "dodgerblue", lwd = 2)
library(lmtest)
bptest(salaries_model_res_trans)
shapiro.test(resid(salaries_model_res_trans))
# Build a linear model with log-transformed response and predictor
salaries_model_res_pred_trans = lm(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries)
summary(salaries_model_res_pred_trans)
par(mfrow = c(1, 1))
# Plot the data
plot(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries,
xlab = "Log of Years of Experience",
ylab = "Log of Total Yearly Compensation",
main = "Scatter plot with fitted line for log-transformed response/predict model")
# Add the fitted line
abline(salaries_model_res_pred_trans, col = "red")
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(salaries_model_res_pred_trans), resid(salaries_model_res_pred_trans), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(salaries_model_res_pred_trans), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(salaries_model_res_pred_trans), col = "dodgerblue", lwd = 2)
library(lmtest)
bptest(salaries_model_res_pred_trans)
shapiro.test(resid(salaries_model_res_pred_trans))
# Create a model with transformed predictors
salaries_model_pred_trans = lm(totalyearlycompensation ~ yearsofexperience + I(yearsofexperience^2), data = salaries)
# Summarize the model
summary(salaries_model_pred_trans)
par(mfrow = c(1, 1))
# Plot the data
plot(totalyearlycompensation ~ yearsofexperience, data = salaries,
xlab = "Years of Experience",
ylab = "Total Yearly Compensation",
main = "Scatter plot with polynomial fit")
# Add the polynomial fit
years = seq(min(salaries$yearsofexperience), max(salaries$yearsofexperience), length.out = 100)
predicted_compensation = predict(salaries_model_pred_trans, newdata = data.frame(yearsofexperience = years))
lines(years, predicted_compensation, col = "red", lwd = 2)
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(salaries_model_pred_trans), resid(salaries_model_pred_trans), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(salaries_model_pred_trans), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(salaries_model_pred_trans), col = "dodgerblue", lwd = 2)
library(lmtest)
bptest(salaries_model_pred_trans)
shapiro.test(resid(salaries_model_pred_trans))
plot(rmse_train, rmse_tst)
plot(rmse_trn, rmse_tst)
library(car)
library(lmtest)
library(boot)
library(caret)
set.seed(22)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#Start with additive model with all predictors. Log response to improve normality
salaries_model = lm(log(totalyearlycompensation) ~ ., data = trn)
#Perform backwards AIC to select significant predictors
salaries_model_back_aic = step(salaries_model, direction = "backward", trace = 0)
#Check for collinearity
car::vif(salaries_model_back_aic) #Nothing too bad
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Run diagnostics
summary(aic_model_fix)$adj
#get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
#Plot diagnostics
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
#Loading dataset
library(readr)
library(stringr)
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")
#Remove NA records
salaries = na.omit(salaries)
#Create new column 'State' which converts location to just the state abbreviation
salaries$state = str_sub(salaries$location,-2,-1)
#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)
#Feel free to uncomment until we get confirmation from TA on removing columns or not
#Variable Selection: Removed variables that contained redundant information
#salaries = subset(salaries, select = c(company, level, title,
#                                       totalyearlycompensation,
#                                       yearsofexperience, yearsatcompany, tag,
#                                       basesalary, stockgrantvalue, bonus,
#                                       gender, Race, Education, state))
#Variable Selection: Removed variables that contained redundant information
salaries = subset(salaries, select = c(company, level, title,
totalyearlycompensation,
yearsofexperience, yearsatcompany, tag,
gender, Race, Education, state))
#salaries = subset(salaries, salaries$totalyearlycompensation < 1000000) #testing out removing salaries over 1mil (6 records)
library(car)
library(lmtest)
library(boot)
library(caret)
set.seed(22)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#Start with additive model with all predictors. Log response to improve normality
salaries_model = lm(log(totalyearlycompensation) ~ ., data = salaries)
#Perform backwards AIC to select significant predictors
salaries_model_back_aic = step(salaries_model, direction = "backward", trace = 0)
#Check for collinearity
car::vif(salaries_model_back_aic) #Nothing too bad
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Run diagnostics
summary(aic_model_fix)$adj
#get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
#Plot diagnostics
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
#Plotting percent error
actual_salary = tst$totalyearlycompensation
library(car)
library(lmtest)
library(boot)
library(caret)
set.seed(22)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#Split the data 70-30 train and test set
#trn_idx = sample(1:nrow(salaries), 1608)
trn_idx = createDataPartition(salaries$level, p = 0.7, list = F)
trn = salaries[trn_idx, ]
tst = salaries[-trn_idx, ]
#Start with additive model with all predictors. Log response to improve normality
salaries_model = lm(log(totalyearlycompensation) ~ ., data = trn)
#Perform backwards AIC to select significant predictors
salaries_model_back_aic = step(salaries_model, direction = "backward", trace = 0)
#Check for collinearity
car::vif(salaries_model_back_aic) #Nothing too bad
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Run diagnostics
summary(aic_model_fix)$adj
#get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
#Plot diagnostics
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
#Plotting percent error
actual_salary = tst$totalyearlycompensation
pred_salary = exp(predict(aic_model_fix, tst))
max_salary = ifelse(max(actual_salary) > max(pred_salary), max(actual_salary), max(pred_salary))
percent_error = mean((abs(pred_salary - actual_salary) / pred_salary)) * 100
plot(pred_salary, actual_salary,
col = "darkgrey",
xlab = "Predicted Salaries Prices",
ylab = "Actual Salaries",
main = "Predicted vs. Actual Salaries",
xlim = c(0, max_salary),
ylim = c(0, max_salary)
)
abline(0, 1, col = "dodgerblue")
#Plotting quartile percent error
first_q = subset(tst, tst$totalyearlycompensation <= max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation > max_salary / 4)
second_q = subset(second_q, tst$totalyearlycompensation <= max_salary / 2)
#Plotting quartile percent error
first_q = subset(tst, tst$totalyearlycompensation <= max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation > max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation <= max_salary / 2)
third_q = subset(tst, tst$totalyearlycompensation > max_salary / 2)
third_q = subset(tst, tst$totalyearlycompensation <= max_salary / (3 / 4))
fourth_q = subset(tst, tst$totalyearlycompensation > max_salary / (3 / 4))
first_pred = exp(predict(aic_model_fix, first_q))
second_pred = exp(predict(aic_model_fix, second_q))
third_pred = exp(predict(aic_model_fix, fhird_q))
#Plotting quartile percent error
first_q = subset(tst, tst$totalyearlycompensation <= max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation > max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation <= max_salary / 2)
third_q = subset(tst, tst$totalyearlycompensation > max_salary / 2)
third_q = subset(tst, tst$totalyearlycompensation <= max_salary / (3 / 4))
fourth_q = subset(tst, tst$totalyearlycompensation > max_salary / (3 / 4))
first_pred = exp(predict(aic_model_fix, first_q))
second_pred = exp(predict(aic_model_fix, second_q))
third_pred = exp(predict(aic_model_fix, third_q))
fourth_pred = exp(predict(aic_model_fix, fourth_q))
first_pe = mean((abs(first_salary - first_q$totalyearlycompensation) / first_salary)) * 100
#Plotting quartile percent error
first_q = subset(tst, tst$totalyearlycompensation <= max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation > max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation <= max_salary / 2)
third_q = subset(tst, tst$totalyearlycompensation > max_salary / 2)
third_q = subset(tst, tst$totalyearlycompensation <= max_salary / (3 / 4))
fourth_q = subset(tst, tst$totalyearlycompensation > max_salary / (3 / 4))
first_pred = exp(predict(aic_model_fix, first_q))
second_pred = exp(predict(aic_model_fix, second_q))
third_pred = exp(predict(aic_model_fix, third_q))
fourth_pred = exp(predict(aic_model_fix, fourth_q))
first_pe = mean((abs(first_pred - first_q$totalyearlycompensation) / first_pred)) * 100
second_pe = mean((abs(second_pred - second_q$totalyearlycompensation) / second_pred)) * 100
third_pe = mean((abs(third_pred - third_q$totalyearlycompensation) / third_pred)) * 100
fourth_pe = mean((abs(fourth_pred - fourth_q$totalyearlycompensation) / fourth_pred)) * 100
#Plotting quartile percent error
first_q = subset(tst, tst$totalyearlycompensation <= max_salary / 4)
second_q = subset(tst, tst$totalyearlycompensation > max_salary / 4)
second_q = subset(second_q, second_q$totalyearlycompensation <= max_salary / 2)
third_q = subset(tst, tst$totalyearlycompensation > max_salary / 2)
third_q = subset(third_q, third_q$totalyearlycompensation <= max_salary / (3 / 4))
fourth_q = subset(tst, tst$totalyearlycompensation > max_salary / (3 / 4))
first_pred = exp(predict(aic_model_fix, first_q))
second_pred = exp(predict(aic_model_fix, second_q))
third_pred = exp(predict(aic_model_fix, third_q))
fourth_pred = exp(predict(aic_model_fix, fourth_q))
first_pe = mean((abs(first_pred - first_q$totalyearlycompensation) / first_pred)) * 100
second_pe = mean((abs(second_pred - second_q$totalyearlycompensation) / second_pred)) * 100
third_pe = mean((abs(third_pred - third_q$totalyearlycompensation) / third_pred)) * 100
fourth_pe = mean((abs(fourth_pred - fourth_q$totalyearlycompensation) / fourth_pred)) * 100
#Loading dataset and importing libraries
library(readr)
library(stringr)
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")
View(salaries)
#Remove NA records
salaries = na.omit(salaries)
#Create new column 'State' which converts location to just the state abbreviation
salaries$state = str_sub(salaries$location,-2,-1)
#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)
#Feel free to uncomment until we get confirmation from TA on removing columns or not
#Variable Selection: Removed variables that contained redundant information
#salaries = subset(salaries, select = c(company, level, title,
#                                       totalyearlycompensation,
#                                       yearsofexperience, yearsatcompany, tag,
#                                       basesalary, stockgrantvalue, bonus,
#                                       gender, Race, Education, state))
#Variable Selection: Removed variables that contained redundant information
salaries = subset(salaries, select = c(company, level, title,
totalyearlycompensation,
yearsofexperience, yearsatcompany, tag,
gender, Race, Education, state))
#salaries = subset(salaries, salaries$totalyearlycompensation < 1000000) #testing out removing salaries over 1mil (6 records)
#Model that has 'totalyearlycompensation' as the response variable and all remaining variables serve as the predictors
salaries_model = lm(totalyearlycompensation ~ ., data = salaries)
#Backward AIC
salaries_model_back_aic = step(salaries_model,
direction = "backward", trace = 0)
#Backward BIC
salaries_model_back_bic = step(salaries_model, direction = "backward",
k = log(nrow(salaries)), trace = 0)
salaries_model_start = lm(totalyearlycompensation ~ 1, data = salaries)
#Loading dataset and importing libraries
library(readr)
library(stringr)
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")
View(salaries)
#Remove NA records
salaries = na.omit(salaries)
#Create new column 'State' which converts location to just the state abbreviation
salaries$state = str_sub(salaries$location,-2,-1)
#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)
#Feel free to uncomment until we get confirmation from TA on removing columns or not
#Variable Selection: Removed variables that contained redundant information
#salaries = subset(salaries, select = c(company, level, title,
#                                       totalyearlycompensation,
#                                       yearsofexperience, yearsatcompany, tag,
#                                       basesalary, stockgrantvalue, bonus,
#                                       gender, Race, Education, state))
#Variable Selection: Removed variables that contained redundant information
salaries = subset(salaries, select = c(company, level, title,
totalyearlycompensation,
yearsofexperience, yearsatcompany, tag,
gender, Race, Education, state))
#salaries = subset(salaries, salaries$totalyearlycompensation < 1000000) #testing out removing salaries over 1mil (6 records)
#Model that has 'totalyearlycompensation' as the response variable and all remaining variables serve as the predictors
salaries_model = lm(totalyearlycompensation ~ ., data = salaries)
#Backward AIC
salaries_model_back_aic = step(salaries_model,
direction = "backward", trace = 0)
#Backward BIC
salaries_model_back_bic = step(salaries_model, direction = "backward",
k = log(nrow(salaries)), trace = 0)
anova(salaries_model_back_bic, salaries_model_back_aic)
#Check for collinearity
car::vif(salaries_model_back_aic) #Nothing too bad
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
anova(salaries_model_back_aic, aic_model_trans)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
anova(aic_model_trans, aic_model_fix)
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_log = lm(log(totalyearlycompensation) ~ company + yearsofexperience + title + level + gender + Race + Education, data = salaries)
anova(salaries_model_back_aic, aic_model_log)
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
anova(salaries_model_log, aic_model_trans)
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_log = lm(log(totalyearlycompensation) ~ company + yearsofexperience + title + level + gender + Race + Education, data = salaries)
anova(salaries_model_back_aic, aic_model_log)
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
anova(aic_model_log, aic_model_trans)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_log = lm(log(totalyearlycompensation) ~ company + yearsofexperience + title + level + gender + Race + Education, data = salaries)
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
anova(aic_model_log, aic_model_trans)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
