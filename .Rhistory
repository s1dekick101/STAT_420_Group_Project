get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + Race) ^ 2  + yearsatcomapny + gender + Education + level, data = salaries)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + Race) ^ 2  + yearsatcompany + gender + Education + level, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + Race) ^ 2  + yearsatcomapny + gender + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + Race) ^ 2  + yearsatcompany + gender + Education + level, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + Race) ^ 2  + yearsatcompany + gender + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (yearsatcompany + yearsofexperience + title + Race) ^ 2  + company + gender + Education + level, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (yearsatcompany + yearsofexperience + title + Race) ^ 2  + company + gender + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (yearsatcompany + yearsofexperience + title) ^ 2  + company + gender + Race + Education + level, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (yearsatcompany + yearsofexperience + title) ^ 2  + company + gender + Race + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (yearsatcompany + yearsofexperience + title + level) ^ 2  + company + gender + Race + Education + level, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (yearsatcompany + yearsofexperience + title + level) ^ 2  + company + gender + Race + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + level) ^ 2  + company + gender + Race + Education + level, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + level) ^ 2  + company + gender + Race + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education + level, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level) + gender + Race + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsatcompany + title) ^ 2 + level + gender + Race + Education, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsatcompany + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + gender) ^ 2 + level + Race + Education, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title + gender) ^ 2 + level + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
View(salaries)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
library(car)
library(lmtest)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#Check for collinearity
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Run diagnostics
summary(aic_model_fix)$adj
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
#Plot diagnostics
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
#Loading dataset
library(readr)
library(stringr)
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")
View(salaries)
#Remove NA records
salaries = na.omit(salaries)
#Create new column 'State' which converts location to just the state abbreviation
salaries$state = str_sub(salaries$location,-2,-1)
#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)
#Feel free to uncomment until we get confirmation from TA on removing columns or not
#Variable Selection: Removed variables that contained redundant information
#salaries = subset(salaries, select = c(company, level, title,
#                                       totalyearlycompensation,
#                                       yearsofexperience, yearsatcompany, tag,
#                                       basesalary, stockgrantvalue, bonus,
#                                       gender, Race, Education, state))
#Variable Selection: Removed variables that contained redundant information
salaries = subset(salaries, select = c(company, level, title,
totalyearlycompensation,
yearsofexperience, yearsatcompany, tag,
gender, Race, Education, state))
#salaries = subset(salaries, salaries$totalyearlycompensation < 1000000) #testing out removing salaries over 1mil (6 records)
#Loading dataset
library(readr)
library(stringr)
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")
View(salaries)
#Remove NA records
salaries = na.omit(salaries)
#Create new column 'State' which converts location to just the state abbreviation
salaries$state = str_sub(salaries$location,-2,-1)
#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)
#Feel free to uncomment until we get confirmation from TA on removing columns or not
#Variable Selection: Removed variables that contained redundant information
#salaries = subset(salaries, select = c(company, level, title,
#                                       totalyearlycompensation,
#                                       yearsofexperience, yearsatcompany, tag,
#                                       basesalary, stockgrantvalue, bonus,
#                                       gender, Race, Education, state))
#Variable Selection: Removed variables that contained redundant information
salaries = subset(salaries, select = c(company, level, title,
totalyearlycompensation,
yearsofexperience, yearsatcompany, tag,
gender, Race, Education, state))
#salaries = subset(salaries, salaries$totalyearlycompensation < 1000000) #testing out removing salaries over 1mil (6 records)
salaries_model = lm(totalyearlycompensation ~ ., data = salaries)
#Didnt use p but n was used for model BIC models
#p = length(coef(salaries_model))
n = length(resid(salaries_model))
#Backward AIC
salaries_model_back_aic = step(salaries_model,
direction = "backward", trace = 0)
#Backward BIC
salaries_model_back_bic = step(salaries_model, direction = "backward",
k = log(n), trace = 0)
salaries_model_start = lm(totalyearlycompensation ~ 1, data = salaries)
#Forward AIC
salaries_model_forw_aic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "forward", trace = 0)
#Forward BIC
salaries_model_forw_bic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "forward", k = log(n), trace = 0)
#Stepwise AIC
salaries_model_both_aic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "both", trace = 0)
#Stepwise BIC
salaries_model_both_bic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "both", k = log(n), trace = 0)
# Create models with transformation and without it.
model_no_trans = lm(totalyearlycompensation ~ level + company + title + yearsofexperience, data = salaries)
model_res_trans = lm(log(totalyearlycompensation) ~ level + company + title + yearsofexperience, data = salaries)
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot - model without transformation
plot(fitted(model_no_trans), resid(model_no_trans), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot - model without transformation
qqnorm(resid(model_no_trans), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_no_trans), col = "dodgerblue", lwd = 2)
# Residuals vs Fitted values plot - model with transformation
plot(fitted(model_res_trans), resid(model_res_trans), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot - model with transformation
qqnorm(resid(model_res_trans), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_res_trans), col = "dodgerblue", lwd = 2)
library(car)
library(lmtest)
library(boot)
library(caret)
set.seed(22)
get_loocv_rmse = function(model){
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#Split the data 70-30 train and test set
#trn_idx = sample(1:nrow(salaries), 1608)
trn_idx = createDataPartition(salaries$level, p = 0.7, list = F)
trn = salaries[trn_idx, ]
tst = salaries[-trn_idx, ]
#Start with additive model with all predictors. Log response to improve normality
salaries_model = lm(log(totalyearlycompensation) ~ ., data = trn)
#Perform backwards AIC to select significant predictors
salaries_model_back_aic = step(salaries_model, direction = "backward", trace = 0)
#Check for collinearity
car::vif(salaries_model_back_aic) #Nothing too bad
#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
#Run diagnostics
summary(aic_model_fix)$adj
#get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)
#Plot diagnostics
par(mfrow = c(1, 2))
# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
#Plotting percent error
actual_salary = tst$totalyearlycompensation
pred_salary = exp(predict(aic_model_fix, tst))
max_salary = ifelse(max(actual_salary) > max(pred_salary), max(actual_salary), max(pred_salary))
percent_error = mean((abs(pred_salary - actual_salary) / pred_salary)) * 100
plot(pred_salary, actual_salary,
col = "darkgrey",
xlab = "Predicted Salaries Prices",
ylab = "Actual Salaries",
main = "Predicted vs. Actual Salaries",
xlim = c(0, max_salary),
ylim = c(0, max_salary)
)
abline(0, 1, col = "dodgerblue")
#Simulation
num_sims = 1000
rmse_trn = rep(0, num_sims)
rmse_tst = rep(0, num_sims)
rmse = function(y, y_hat){
sqrt(sum((y - y_hat) ^ 2) / length(y))
}
for(i in 1:num_sims){
sim_fit = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
rmse_trn[i] = rmse(trn$totalyearlycompensation, exp(predict(sim_fit, trn)))
rmse_tst[i] = rmse(tst$totalyearlycompensation, exp(predict(sim_fit, tst)))
}
mean(rmse_trn)
mean(rmse_tst)
#Predict salaries
#salary_1 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 0, gender = "Female", Race = "Asian", Education = "Master's Degree") #150000
#salary_2 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 2, gender = "Male", Race = "Hispanic", Education = "Bachelor's Degree") #152000
#salary_3 = data.frame(company = "Google", level = "L5", title = "Software Engineer", #yearsofexperience = 9, gender = "Male", Race = "Asian", Education = "PhD") #396000
#exp(predict(aic_model_fix, newdata = salary_1))
#exp(predict(aic_model_fix, newdata = salary_2))
#exp(predict(aic_model_fix, newdata = salary_3))
salaries_model = lm(totalyearlycompensation ~ ., data = salaries)
#Didnt use p but n was used for model BIC models
#p = length(coef(salaries_model))
n = length(resid(salaries_model))
#Backward AIC
salaries_model_back_aic = step(salaries_model,
direction = "backward", trace = 0)
#Backward BIC
salaries_model_back_bic = step(salaries_model, direction = "backward",
k = log(n), trace = 0)
salaries_model_start = lm(totalyearlycompensation ~ 1, data = salaries)
#Forward AIC
salaries_model_forw_aic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "forward", trace = 0)
#Forward BIC
salaries_model_forw_bic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "forward", k = log(n), trace = 0)
#Stepwise AIC
salaries_model_both_aic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "both", trace = 0)
#Stepwise BIC
salaries_model_both_bic = step(salaries_model_start,
scope = totalyearlycompensation ~ company + level +
title + yearsofexperience + yearsatcompany + tag +
gender + Race + Education + state,
direction = "both", k = log(n), trace = 0)
library(lmtest)
salaries_model_add = lm(log(totalyearlycompensation) ~ (yearsatcompany + company + title + state + level) ^ 2, data = salaries)
salaries_model_fix = lm(log(totalyearlycompensation) ~ (yearsatcompany + company + title + state + level) ^ 2, data = salaries, subset = cooks.distance(salaries_model_add) < 4 / length(cooks.distance(salaries_model_add)))
hist(resid(salaries_model_fix))
#salaries_model_int = lm(totalyearlycompensation ~ level * title * yearsofexperience * yearsatcompany, data = salaries)
#anova(salaries_model_add, salaries_model_int)
#summary(salaries_model_int)
#salaries_model_three_int = lm(totalyearlycompensation ~ company * level * yearsofexperience, data = salaries)
qqnorm(resid(salaries_model_fix))
qqline(resid(salaries_model_fix))
plot(fitted(salaries_model_fix), resid(salaries_model_fix))
abline(h=0)
shapiro.test(resid(salaries_model_fix))
bptest(salaries_model_fix)
#anova(salaries_model_add, salaries_model_two_int)
#anova(salaries_model_two_int, salaries_model_three_int)
# # Create a model with transformed predictors
# model_res_trans= lm(log(totalyearlycompensation) ~ yearsofexperience, data = salaries)
# summary(model_res_trans)
#
# # Plot the data
#  plot(log(totalyearlycompensation) ~ yearsofexperience, data = salaries,
#       xlab = "Years of Experience",
#       ylab = "Log of Total Yearly Compensation",
#       main = "Scatter plot with fitted line for log-transformed response model")
#
# # Add the fitted line
# abline(model_res_trans, col = "red")
#
# par(mfrow = c(1, 2))
#
# # Residuals vs Fitted values plot
# plot(fitted(model_res_trans), resid(model_res_trans), col = "grey", pch = 20,
#      xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
# abline(h = 0, col = "darkorange", lwd = 2)
#
# # Q-Q plot
# qqnorm(resid(model_res_trans), main = "Normal Q-Q Plot", col = "darkgrey")
# qqline(resid(model_res_trans), col = "dodgerblue", lwd = 2)
#
# library(lmtest)
# bptest(model_res_trans)
# shapiro.test(resid(model_res_trans))
#
# # Build a linear model with log-transformed response and predictor
# salaries_model_res_pred_trans = lm(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries)
#
# summary(salaries_model_res_pred_trans)
#
# par(mfrow = c(1, 1))
#
# # Plot the data
#  plot(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries,
#       xlab = "Log of Years of Experience",
#       ylab = "Log of Total Yearly Compensation",
#       main = "Scatter plot with fitted line for log-transformed response/predict model")
#
# # Add the fitted line
# abline(salaries_model_res_pred_trans, col = "red")
#
# par(mfrow = c(1, 2))
#
# # Residuals vs Fitted values plot
# plot(fitted(salaries_model_res_pred_trans), resid(salaries_model_res_pred_trans), col = "grey", pch = 20,
#      xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
# abline(h = 0, col = "darkorange", lwd = 2)
#
# # Q-Q plot
# qqnorm(resid(salaries_model_res_pred_trans), main = "Normal Q-Q Plot", col = "darkgrey")
# qqline(resid(salaries_model_res_pred_trans), col = "dodgerblue", lwd = 2)
#
# library(lmtest)
# bptest(salaries_model_res_pred_trans)
# shapiro.test(resid(salaries_model_res_pred_trans))
#
#
# # Create a model with transformed predictors
# salaries_model_pred_trans = lm(totalyearlycompensation ~ yearsofexperience + I(yearsofexperience^2), data = salaries)
#
# # Summarize the model
# summary(salaries_model_pred_trans)
#
# par(mfrow = c(1, 1))
#
# # Plot the data
# plot(totalyearlycompensation ~ yearsofexperience, data = salaries,
#      xlab = "Years of Experience",
#      ylab = "Total Yearly Compensation",
#      main = "Scatter plot with polynomial fit")
#
# # Add the polynomial fit
# years = seq(min(salaries$yearsofexperience), max(salaries$yearsofexperience), length.out = 100)
# predicted_compensation = predict(salaries_model_pred_trans, newdata = data.frame(yearsofexperience = years))
#
# lines(years, predicted_compensation, col = "red", lwd = 2)
#
# par(mfrow = c(1, 2))
#
# # Residuals vs Fitted values plot
# plot(fitted(salaries_model_pred_trans), resid(salaries_model_pred_trans), col = "grey", pch = 20,
#      xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
# abline(h = 0, col = "darkorange", lwd = 2)
#
# # Q-Q plot
# qqnorm(resid(salaries_model_pred_trans), main = "Normal Q-Q Plot", col = "darkgrey")
# qqline(resid(salaries_model_pred_trans), col = "dodgerblue", lwd = 2)
#
# library(lmtest)
# bptest(salaries_model_pred_trans)
# shapiro.test(resid(salaries_model_pred_trans))
