---
title: "Data Analysis of Tech. Salaries at Amazon and Google"
author: "The Statisticians"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: readable
    toc: yes
urlcolor: cyan
---

### <u>Introduction</u>

As data science students, we're intrigued by the compensation structures within leading tech companies like Amazon and Google. To delve into this interest, we're undertaking a project to analyze a dataset of 12,647 salary records from these companies, with an aim to construct a predictive model for annual total compensation.

This dataset, `Levels-Fyi-Salary-Data-Cleaned.csv`, provides a unique and in-depth perspective into the salary structures of numerous top-tier companies. The dataset encompasses 28 variables, covering a variety of aspects such as company, level, title, location, years of experience, years at the company, and various demographic factors including gender and education level. Of particular interest to us is the `totalyearlycompensation` variable, which will serve as the response variable in our analysis.

Our interest in this dataset is driven by our curiosity about the factors that influence annual compensation in top companies. We aim to identify the key variables and their interactions that significantly impact salary levels. This would provide insights that could be invaluable to a variety of stakeholders, from job seekers and employees to human resource professionals and policymakers.

The primary goal of creating a model with this data is to predict the `totalyearlycompensation` based on a set of predictor variables. This model will help us understand how different variables, including categorical ones like `company` and numeric ones such as `yearsofexperience` and `basesalary`, contribute to the total yearly compensation of an employee. Through this process, we also hope to identify potential trends, anomalies, or points of interest that could provide further avenues for exploration and analysis.

***

### <u>Methods</u>

-- We choose the following model... `(PLACE HOLDER)`

```{r message=FALSE, warning=FALSE}
#Loading dataset
library(readr)
library(stringr)
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")

#Remove NA records
salaries = na.omit(salaries)

#Create new column 'State' which only shows the Convert to State
salaries$state = str_sub(salaries$location,-2,-1)

#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)

#Feel free to uncomment until we get confirmation from TA on removing columns or not
#Variable Selection: Removed variables that contained redundant information 
#salaries = subset(salaries, select = c(company, level, title,
#                                       totalyearlycompensation,
#                                       yearsofexperience, yearsatcompany, tag,
#                                       basesalary, stockgrantvalue, bonus,
#                                       gender, Race, Education, state))

#Variable Selection: Removed variables that contained redundant information 
salaries = subset(salaries, select = c(company, level, title,
                                       totalyearlycompensation,
                                       yearsofexperience, yearsatcompany, tag,
                                       gender, Race, Education, state))
```

```{r}
# Placeholder and we will just put the model we actually use here 
model = 2
```


***

### <u>Results</u>


***

### <u>Discussion</u>


***

### <u>Appendix</u> 

Linear Model that includes all predictors
```{r}
salaries_model = lm(totalyearlycompensation ~ ., data = salaries)
```

AIC/BIC Models
```{r}
#Didnt use p but n was used for model BIC models  
#p = length(coef(salaries_model))
n = length(resid(salaries_model))

#Backward AIC
salaries_model_back_aic = step(salaries_model,
                               direction = "backward", trace = 0)

#Backward BIC
salaries_model_back_bic = step(salaries_model, direction = "backward",
                               k = log(n), trace = 0)

salaries_model_start = lm(totalyearlycompensation ~ 1, data = salaries)

#Forward AIC
salaries_model_forw_aic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "forward", trace = 0)

#Forward BIC
salaries_model_forw_bic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "forward", k = log(n), trace = 0)

#Stepwise AIC
salaries_model_both_aic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "both", trace = 0)

#Stepwise BIC
salaries_model_both_bic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "both", k = log(n), trace = 0)


#AIC derives a model using 9 of the 10 initial inputted predictors
#salaries_model_back_aic
#salaries_model_forw_aic
#salaries_model_both_aic

#BIC derives a model using 4 of the 10 initial inputted predictors
#salaries_model_back_bic
#salaries_model_forw_bic
#salaries_model_both_bic
```

Model 2 
```{r}
library(lmtest)
# salaries_model_add = lm(log(totalyearlycompensation) ~ yearsatcompany * title * state * level, data = salaries)
# 
# summary(salaries_model_add)
# 
# salaries_model_fix = lm(log(totalyearlycompensation) ~ yearsatcompany * title * state * level, data = salaries, subset = cooks.distance(salaries_model_add) < 4 / length(cooks.distance(salaries_model_add)))
# 
# hist(resid(salaries_model_fix))
# 
# #salaries_model_int = lm(totalyearlycompensation ~ level * title * yearsofexperience * yearsatcompany, data = salaries)
# 
# #anova(salaries_model_add, salaries_model_int)
# #summary(salaries_model_int)
# 
# #salaries_model_three_int = lm(totalyearlycompensation ~ company * level * yearsofexperience, data = salaries)
# 
# qqnorm(resid(salaries_model_fix))
# qqline(resid(salaries_model_fix))
# plot(fitted(salaries_model_fix), resid(salaries_model_fix))
# abline(h=0)
# shapiro.test(resid(salaries_model_fix))
# bptest(salaries_model_fix)
#anova(salaries_model_add, salaries_model_two_int)

#anova(salaries_model_two_int, salaries_model_three_int)

```


Predictor/Response Transforamtion Models
```{r}
# Create a model with transformed predictors
salaries_model_res_trans= lm(log(totalyearlycompensation) ~ yearsofexperience, data = salaries)
summary(salaries_model_res_trans)

# Plot the data
 plot(log(totalyearlycompensation) ~ yearsofexperience, data = salaries,
      xlab = "Years of Experience",
      ylab = "Log of Total Yearly Compensation",
      main = "Scatter plot with fitted line for log-transformed response model")

# Add the fitted line
abline(salaries_model_res_trans, col = "red")

par(mfrow = c(1, 2))

# Residuals vs Fitted values plot
plot(fitted(salaries_model_res_trans), resid(salaries_model_res_trans), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

# Q-Q plot
qqnorm(resid(salaries_model_res_trans), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(salaries_model_res_trans), col = "dodgerblue", lwd = 2)

library(lmtest)
bptest(salaries_model_res_trans)
shapiro.test(resid(salaries_model_res_trans))

# Build a linear model with log-transformed response and predictor

salaries_model_res_pred_trans = lm(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries)

summary(salaries_model_res_pred_trans)

par(mfrow = c(1, 1))

# Plot the data
 plot(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries,
      xlab = "Log of Years of Experience",
      ylab = "Log of Total Yearly Compensation",
      main = "Scatter plot with fitted line for log-transformed response/predict model")

# Add the fitted line
abline(salaries_model_res_pred_trans, col = "red")

par(mfrow = c(1, 2))

# Residuals vs Fitted values plot
plot(fitted(salaries_model_res_pred_trans), resid(salaries_model_res_pred_trans), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

# Q-Q plot
qqnorm(resid(salaries_model_res_pred_trans), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(salaries_model_res_pred_trans), col = "dodgerblue", lwd = 2)

library(lmtest)
bptest(salaries_model_res_pred_trans)
shapiro.test(resid(salaries_model_res_pred_trans))

```

Model Selection
```{r}

```



By: Nicholas O. Brown, Ellie Jung, Angela Mei

***

