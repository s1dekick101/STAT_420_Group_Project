---
title: "Data Analysis of Tech. Salaries at Amazon and Google"
author: "The Statisticians"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: readable
    toc: yes
urlcolor: cyan
---

### <u>Introduction</u>

As data science students, we're intrigued by the compensation structures within leading tech companies like Amazon and Google. To delve into this interest, we're undertaking a project to analyze a dataset of 12,647 salary records from these companies, with an aim to construct a predictive model for annual total compensation.

This dataset, `Levels-Fyi-Salary-Data-Cleaned.csv`, provides a unique and in-depth perspective into the salary structures of numerous top-tier companies. The dataset encompasses 28 variables, covering a variety of aspects such as company, level, title, location, years of experience, years at the company, and various demographic factors including gender and education level. Of particular interest to us is the `totalyearlycompensation` variable, which will serve as the response variable in our analysis.

Our interest in this dataset is driven by our curiosity about the factors that influence annual compensation in top companies. We aim to identify the key variables and their interactions that significantly impact salary levels. This would provide insights that could be invaluable to a variety of stakeholders, from job seekers and employees to human resource professionals and policymakers.

The primary goal of creating a model with this data is to predict the `totalyearlycompensation` based on a set of predictor variables. This model will help us understand how different variables, including categorical ones like `company` and numeric ones such as `yearsofexperience` and `basesalary`, contribute to the total yearly compensation of an employee. Through this process, we also hope to identify potential trends, anomalies, or points of interest that could provide further avenues for exploration and analysis.

***

### <u>Methods</u>

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(digits = 4)
```

#### Description of the original data file
- company: The company the employee works for
- level: The rank of the employee
- title: The job title of the employee
- totalyearlycompensation: The total yearly compensation of the employee
- yearsofexperience: The total years of experience of the employee
- yearsatcompany: The years at the company of the employee
- tag: A tag for additional information
- gender: The gender of the employee
- Race: The race of the employee
- Education: The education level of the employee
- state: The state of the work location

#### Data Pre-Processing 

```{r message=FALSE, warning=FALSE}
#Loading dataset and importing libraries
library(readr)
library(stringr)
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")

#Is this necessary?
#head(salaries)

#Remove NA records
salaries = na.omit(salaries)

#Create new column 'State' which converts location to just the state abbreviation
salaries$state = str_sub(salaries$location,-2,-1)

#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)

#Feel free to uncomment until we get confirmation from TA on removing columns or not
#Variable Selection: Removed variables that contained redundant information 
#salaries = subset(salaries, select = c(company, level, title,
#                                       totalyearlycompensation,
#                                       yearsofexperience, yearsatcompany, tag,
#                                       basesalary, stockgrantvalue, bonus,
#                                       gender, Race, Education, state))

#Variable Selection: Removed variables that contain redundant information or lack of information 
salaries = subset(salaries, select = c(company, level, title,
                                       totalyearlycompensation,
                                       yearsofexperience, yearsatcompany, tag,
                                       gender, Race, Education))

#salaries = subset(salaries, salaries$totalyearlycompensation < 1000000) #testing out removing salaries over 1mil (6 records)
```

#### Model Selection Proccess 

1. Selection of Predictors Using AIC/BIC Methodology

```{r}
#Model w/ 'totalyearlycompensation' as the response variable and all remaining variables as the predictors
salaries_model = lm(totalyearlycompensation ~ ., data = salaries)

#Backward AIC
salaries_model_back_aic = step(salaries_model,
                               direction = "backward", trace = 0)

#Backward BIC
salaries_model_back_bic = step(salaries_model, direction = "backward",
                               k = log(nrow(salaries)), trace = 0)
```

-- Using the AIC searching method, we derive a model that uses 7 of the 9 given predictors

   Selected Predictors: **<u>`r all.vars(formula(salaries_model_back_aic)[-1])`</u>**
  
-- Using the BIC searching method, we derive a model that uses 4 of the 9 given predictors

   Selected Predictors: **<u>`r all.vars(formula(salaries_model_back_bic)[-1])`</u>**

Please see Appendix for forward and stepwise procedures.
  

```{r}
#Performing anova test on the AIC and BIC models 
anova(salaries_model_back_bic, salaries_model_back_aic)
```

Based upon the results of our test, we calculate a p-value of `r anova(salaries_model_back_bic, salaries_model_back_aic)[2, "Pr(>F)"]` meaning that we reject the BIC model and accept the larger AIC model (`salaries_model_back_aic`). 

```{r}
#Checking the collinearity of the AIC model 
car::vif(salaries_model_back_aic) #Nothing too bad
```
  
2. Performing Model Transformations

```{r}
#Performing log response transformation to improve linearity
aic_model_log = lm(log(totalyearlycompensation) ~ company + yearsofexperience + title + level + gender + Race + Education, data = salaries)

#Add interactions to predictors to improve normality, linearity and constant variance
aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)

anova(aic_model_log, aic_model_trans)
```

Based upon the results of our anova test, we calculated a p-value of < 2.2e-16 meaning that we reject the log model (`aic_model_log`) and would prefer the full transformation model(`aic_model_trans`). 

```{r}
#Removing influential data points from the model
aic_model_final = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))
```

###############################################################

3. Run Diagnostics

```{r message=FALSE, warning=FALSE}
#Importing libraries
library(car)
library(lmtest)
library(boot)
library(caret)
library(knitr)

#LOOCV RMSE Function - ... 
get_loocv_rmse = function(model){
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

#Diagnostics.. not sure if we should still include it now that its results are now in a table 
#summary(aic_model_final)$adj 
#get_loocv_rmse(aic_model_final)
#shapiro.test(resid(aic_model_final))$p.value
#test = bptest(aic_model_final)$p.value

#Diagnostics Table 
row_names = c("Adjusted R", "LOOCV-RMSE", "Shapiro Test", "BP Test")
column_names = c(" ", "Model")
diagnostics = c(summary(aic_model_final)$adj,get_loocv_rmse(aic_model_final),
                (shapiro.test(resid(aic_model_final))$p.value),
                bptest(aic_model_final)$p.value)
df_results = data.frame(row_names, diagnostics)
kable(df_results, "simple", col.names = column_names)
```

###############################################################

4. Testing model using test / train 

```{r, message = FALSE, warning = FALSE}
#Importing library 
library(caret)

#Split the data 70-30 train and test set
#trn_idx = sample(1:nrow(salaries), 1608)
set.seed(22)
trn_idx = createDataPartition(salaries$level, p = 0.7, list = F)
trn = salaries[trn_idx, ]
tst = salaries[-trn_idx, ]

#Calculating average percent error
actual_salary = tst$totalyearlycompensation
pred_salary = exp(predict(aic_model_final, tst))
max_salary = ifelse(max(actual_salary) > max(pred_salary), max(actual_salary), max(pred_salary))
percent_error = mean((abs(pred_salary - actual_salary) / pred_salary)) * 100
```

5. Model simulation  

```{r, message = FALSE, warning = FALSE}

#Simulation Set-Up
num_sims = 1000
rmse_trn = rep(0, num_sims)
rmse_tst = rep(0, num_sims)

#RMSE Function - ... 
rmse = function(y, y_hat){
  sqrt(sum((y - y_hat) ^ 2) / length(y))
}

for(i in 1:num_sims){
  sim_fit = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = trn)
  
  rmse_trn[i] = rmse(trn$totalyearlycompensation, exp(predict(sim_fit, trn)))
  rmse_tst[i] = rmse(tst$totalyearlycompensation, exp(predict(sim_fit, tst)))
}

#Think we should comment this out and call this out in the results section
mean(rmse_trn)
mean(rmse_tst)
```


5. Model Predictions  

```{r, message = FALSE, warning = FALSE}

#Predict salaries
#salary_1 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 0, gender = "Female", Race = "Asian", Education = "Master's Degree") #150000

#salary_2 = data.frame(company = "Amazon", level = "L4", title = "Software Engineer", #yearsofexperience = 2, gender = "Male", Race = "Hispanic", Education = "Bachelor's Degree") #152000

#salary_3 = data.frame(company = "Google", level = "L5", title = "Software Engineer", #yearsofexperience = 9, gender = "Male", Race = "Asian", Education = "PhD") #396000

#exp(predict(aic_model_final, newdata = salary_1))
#exp(predict(aic_model_final, newdata = salary_2))
#exp(predict(aic_model_final, newdata = salary_3))
```
***

### <u>Results</u>

Based on our results, we expect our model to deliver a `r 100 - percent_error`% accuracy rating.

Diagnostics
```{r}
#Plot diagnostics
par(mfrow = c(1, 2))

# Residuals vs Fitted values plot
plot(fitted(aic_model_final), resid(aic_model_final), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

# Q-Q plot
qqnorm(resid(aic_model_final), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_final), col = "dodgerblue", lwd = 2)

#Insert table with other diagnostics, r.squared etc. 
```

Percent Error - Fitted vs. Actual Plot
```{r}
plot(pred_salary, actual_salary,
     col = "darkgrey",
     xlab = "Predicted Salaries Prices",
     ylab = "Actual Salaries",
     main = "Predicted vs. Actual Salaries",
     xlim = c(0, max_salary),
     ylim = c(0, max_salary)
     )
abline(0, 1, col = "dodgerblue")

```


Predictions?
```{r}

```

***

### <u>Discussion</u>


***

### <u>Appendix</u> 


```{r warning=FALSE}
#HERE FOR NOW BUT WILL BE CLEANED UP
salaries = read_csv("Levels-Fyi-Salary-Data-Cleaned-US.csv")

#Remove NA records
salaries = na.omit(salaries)

#Create new column 'State' which converts location to just the state abbreviation
salaries$state = str_sub(salaries$location,-2,-1)

#Changing some predictors to factors
salaries$company = as.factor(salaries$company)
salaries$level = as.factor(salaries$level)
salaries$title = as.factor(salaries$title)
salaries$location = as.factor(salaries$location)
salaries$state = as.factor(salaries$state)
salaries$gender = as.factor(salaries$gender)
salaries$Race = as.factor(salaries$Race)
salaries$Education = as.factor(salaries$Education)

#Variable Selection: Removed variables that contain redundant information or lack of information 
salaries = subset(salaries, select = c(company, level, title,
                                       totalyearlycompensation,
                                       yearsofexperience, yearsatcompany, tag,
                                       gender, Race, state, Education))
```


Linear Model that includes all predictors
```{r}
salaries_model = lm(totalyearlycompensation ~ ., data = salaries)
```

AIC/BIC Models
```{r}
#Didnt use p but n was used for model BIC models  
#p = length(coef(salaries_model))
n = length(resid(salaries_model))

#Backward AIC
salaries_model_back_aic = step(salaries_model,
                               direction = "backward", trace = 0)

#Backward BIC
salaries_model_back_bic = step(salaries_model, direction = "backward",
                               k = log(nrow(salaries)), trace = 0)

salaries_model_start = lm(totalyearlycompensation ~ 1, data = salaries)

#Forward AIC
salaries_model_forw_aic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "forward", trace = 0)

#Forward BIC
salaries_model_forw_bic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "forward", k = log(nrow(salaries)), trace = 0)

#Stepwise AIC
salaries_model_both_aic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "both", trace = 0)

#Stepwise BIC
salaries_model_both_bic = step(salaries_model_start, 
                               scope = totalyearlycompensation ~ company + level +
                               title + yearsofexperience + yearsatcompany + tag +
                               gender + Race + Education + state,
                               direction = "both", k = log(nrow(salaries)), trace = 0)
```


-- Using AIC searching methods we derive the same model that use 9 of the 10 given predictors

  AIC Selected Predictors: **<u>`r all.vars(formula(salaries_model_both_aic)[-1])`</u>**
  
-- Using BIC searching methods we derive the same model that use 4 of the 10 given predictors

  AIC Selected Predictors: **<u>`r all.vars(formula(salaries_model_both_bic)[-1])`</u>**


Model 2 
```{r}
library(lmtest)
salaries_model_add = lm(log(totalyearlycompensation) ~ (yearsatcompany + company + title + state + level) ^ 2, data = salaries)
 
salaries_model_fix = lm(log(totalyearlycompensation) ~ (yearsatcompany + company + title + state + level) ^ 2, data = salaries, subset = cooks.distance(salaries_model_add) < 4 / length(cooks.distance(salaries_model_add)))

hist(resid(salaries_model_fix))
 
#salaries_model_int = lm(totalyearlycompensation ~ level * title * yearsofexperience * yearsatcompany, data = salaries)

#anova(salaries_model_add, salaries_model_int)
#summary(salaries_model_int)

#salaries_model_three_int = lm(totalyearlycompensation ~ company * level * yearsofexperience, data = salaries)

qqnorm(resid(salaries_model_fix))
qqline(resid(salaries_model_fix))
plot(fitted(salaries_model_fix), resid(salaries_model_fix))
abline(h=0)
shapiro.test(resid(salaries_model_fix))
bptest(salaries_model_fix)
#anova(salaries_model_add, salaries_model_two_int)

#anova(salaries_model_two_int, salaries_model_three_int)

```


Predictor/Response Transforamtion Models
```{r}
# # Create a model with transformed predictors
# model_res_trans= lm(log(totalyearlycompensation) ~ yearsofexperience, data = salaries)
# summary(model_res_trans)
# 
# # Plot the data
#  plot(log(totalyearlycompensation) ~ yearsofexperience, data = salaries,
#       xlab = "Years of Experience",
#       ylab = "Log of Total Yearly Compensation",
#       main = "Scatter plot with fitted line for log-transformed response model")
# 
# # Add the fitted line
# abline(model_res_trans, col = "red")
# 
# par(mfrow = c(1, 2))
# 
# # Residuals vs Fitted values plot
# plot(fitted(model_res_trans), resid(model_res_trans), col = "grey", pch = 20,
#      xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
# abline(h = 0, col = "darkorange", lwd = 2)
# 
# # Q-Q plot
# qqnorm(resid(model_res_trans), main = "Normal Q-Q Plot", col = "darkgrey")
# qqline(resid(model_res_trans), col = "dodgerblue", lwd = 2)
# 
# library(lmtest)
# bptest(model_res_trans)
# shapiro.test(resid(model_res_trans))
# 
# # Build a linear model with log-transformed response and predictor
# salaries_model_res_pred_trans = lm(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries)
# 
# summary(salaries_model_res_pred_trans)
# 
# par(mfrow = c(1, 1))
# 
# # Plot the data
#  plot(log(totalyearlycompensation) ~ log(yearsofexperience+1), data = salaries,
#       xlab = "Log of Years of Experience",
#       ylab = "Log of Total Yearly Compensation",
#       main = "Scatter plot with fitted line for log-transformed response/predict model")
# 
# # Add the fitted line
# abline(salaries_model_res_pred_trans, col = "red")
# 
# par(mfrow = c(1, 2))
# 
# # Residuals vs Fitted values plot
# plot(fitted(salaries_model_res_pred_trans), resid(salaries_model_res_pred_trans), col = "grey", pch = 20,
#      xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
# abline(h = 0, col = "darkorange", lwd = 2)
# 
# # Q-Q plot
# qqnorm(resid(salaries_model_res_pred_trans), main = "Normal Q-Q Plot", col = "darkgrey")
# qqline(resid(salaries_model_res_pred_trans), col = "dodgerblue", lwd = 2)
# 
# library(lmtest)
# bptest(salaries_model_res_pred_trans)
# shapiro.test(resid(salaries_model_res_pred_trans))
# 
# 
# # Create a model with transformed predictors
# salaries_model_pred_trans = lm(totalyearlycompensation ~ yearsofexperience + I(yearsofexperience^2), data = salaries)
# 
# # Summarize the model
# summary(salaries_model_pred_trans)
# 
# par(mfrow = c(1, 1))
# 
# # Plot the data
# plot(totalyearlycompensation ~ yearsofexperience, data = salaries,
#      xlab = "Years of Experience",
#      ylab = "Total Yearly Compensation",
#      main = "Scatter plot with polynomial fit")
# 
# # Add the polynomial fit
# years = seq(min(salaries$yearsofexperience), max(salaries$yearsofexperience), length.out = 100)
# predicted_compensation = predict(salaries_model_pred_trans, newdata = data.frame(yearsofexperience = years))
# 
# lines(years, predicted_compensation, col = "red", lwd = 2)
# 
# par(mfrow = c(1, 2))
# 
# # Residuals vs Fitted values plot
# plot(fitted(salaries_model_pred_trans), resid(salaries_model_pred_trans), col = "grey", pch = 20,
#      xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
# abline(h = 0, col = "darkorange", lwd = 2)
# 
# # Q-Q plot
# qqnorm(resid(salaries_model_pred_trans), main = "Normal Q-Q Plot", col = "darkgrey")
# qqline(resid(salaries_model_pred_trans), col = "dodgerblue", lwd = 2)
# 
# library(lmtest)
# bptest(salaries_model_pred_trans)
# shapiro.test(resid(salaries_model_pred_trans))

```

Model Selection
```{r}
library(car)
library(lmtest)

get_loocv_rmse = function(model){
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#start_model = lm(log(totalyearlycompensation) ~ . ^ 2, data = salaries)
#aic_model = step(start_model, direction = "backward", trace = 0)
#Check for collinearity
car::vif(salaries_model_both_aic) #Nothing too bad, perhaps level is high

#Transform log response and add interactions to predictors to improve normality, linearity, constant variance, adjusted R-squared, and loocv-RMSE
aic_model_trans_2 = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + level + gender + Race + Education, data = salaries)

aic_model_trans = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + gender + Race + Education + level, data = salaries)

anova(aic_model_trans, aic_model_trans_2)

#Remove influential data from the model
aic_model_fix = lm(log(totalyearlycompensation) ~ (company + yearsofexperience + title) ^ 2 + gender + Race + Education + level, data = salaries, subset = cooks.distance(aic_model_trans) < 4 / length(cooks.distance(aic_model_trans)))

#Run diagnostics
summary(aic_model_fix)$adj 
get_loocv_rmse(aic_model_fix)
shapiro.test(resid(aic_model_fix))
bptest(aic_model_fix)

#Plot diagnostics
par(mfrow = c(1, 2))

# Residuals vs Fitted values plot
plot(fitted(aic_model_fix), resid(aic_model_fix), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

# Q-Q plot
qqnorm(resid(aic_model_fix), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model_fix), col = "dodgerblue", lwd = 2)
```

By: Nicholas O. Brown, Ellie Jung, Angela Mei

***

